---

# IAM Role must be created prior.
# Guide https://docs.aws.amazon.com/eks/latest/userguide/getting-started.html
# https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2018-08-30/amazon-eks-vpc-sample    

- name: Create a VPC for the EKS cluster.
  cloudformation:
    stack_name: '{{eks_vpc_stack_name}}'
    state: present
    region: '{{ec2_region}}'
    template_body: "{{ lookup('template', 'amazon-eks-vpc-sample.yaml') }}"
    template_parameters:
      VpcBlock: '{{eks_vpc_block}}'
      Subnet01Block: '{{eks_subnet1_block}}'
      Subnet02Block: '{{eks_subnet2_block}}'
      Subnet03Block: '{{eks_subnet3_block}}'
  register: eks_vpc_out
        
- debug: var=eks_vpc_out.stack_outputs.SecurityGroups
- debug: var=eks_vpc_out.stack_outputs.SubnetIds
- debug: var=eks_vpc_out.stack_outputs.VpcId

- include_tasks: installtools.yml
  
- name: Check whether EKS Cluster is present.
  shell: aws eks describe-cluster --name {{eks_name}} --region {{ec2_region}}
  ignore_errors: yes
  changed_when: false
  register: eks_cluster_details

- name: Create EKS if cluster is absent.
  shell: |
    aws eks create-cluster --region {{ec2_region}} --name {{eks_name}} --role-arn {{eks_iam_role}} \
    --resources-vpc-config subnetIds={{eks_vpc_out.stack_outputs.SubnetIds}},securityGroupIds={{eks_vpc_out.stack_outputs.SecurityGroups}}
  when: eks_cluster_details is failed

- name: Get EKS Cluster state. It usually takes about 10 minutes to become ACTIVE.
  shell: aws eks describe-cluster --name {{eks_name}} --region {{ec2_region}} --query 'cluster.status'
  register: eks_cluster_details
  retries: 20
  delay: 30
  until: "'ACTIVE' in eks_cluster_details.stdout"

- name: Get Cluster Endpoint.
  shell: aws eks describe-cluster --name {{eks_name}} --region {{ec2_region}}  --query cluster.endpoint --output text
  changed_when: false
  register: eks_endpoint

- name: Retrieve certificateAuthority data for the cluster.
  shell: aws eks describe-cluster --name {{eks_name}} --region {{ec2_region}}  --query cluster.certificateAuthority.data --output text
  changed_when: false
  register: eks_ca_data

- name: Create kubeconfig dir for AWS EKS.
  file:
    path: '{{kubeconfig_dir}}'
    state: directory

- name: Create kubeconfig file
  copy:
    dest: '{{kubeconfig_dir}}/config-{{eks_name}}'
    content: |
      apiVersion: v1
      clusters:
      - cluster:
          server: {{eks_endpoint.stdout}}
          certificate-authority-data: {{eks_ca_data.stdout}}
        name: kubernetes
      contexts:
      - context:
          cluster: kubernetes
          user: aws
        name: aws
      current-context: aws
      kind: Config
      preferences: {}
      users:
      - name: aws
        user:
          exec:
            apiVersion: client.authentication.k8s.io/v1alpha1
            command: aws-iam-authenticator
            args:
              - "token"
              - "-i"
              - "{{eks_name}}"
              # - "-r"
              # - "<role-arn>"
              # env:
              # - name: AWS_PROFILE
              #   value: "<aws-profile>"

- name: Update bashrc with Kubeconfig path to ensure that Kubectl picks it up.
  lineinfile:
    path: /home/{{lookup('env','USER')}}/.bashrc
    line: 'export KUBECONFIG=$KUBECONFIG:{{kubeconfig_dir}}/config-{{eks_name}}'

- name: Create Kubernetes Dashboard. This is automatically declarative.
  shell: kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
  register: eks_dashboard_out
  changed_when: "'created' in eks_dashboard_out.stdout"

- name: Create Kubernetes dashboard admin user config file.
  copy:
    dest: /tmp/eks_admin_serviceaccount.yml
    content: |
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: eks-admin
        namespace: kube-system

- name: Create Kubernetes dashboard cluster role binding config file.
  copy:
    dest: /tmp/eks_admin_clusterrolebinding.yml
    content: |
      apiVersion: rbac.authorization.k8s.io/v1beta1
      kind: ClusterRoleBinding
      metadata:
        name: eks-admin
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: cluster-admin
      subjects:
      - kind: ServiceAccount
        name: eks-admin
        namespace: kube-system
    
- name: Create Kubernetes dashboard admin user.
  shell: |
    kubectl apply -f /tmp/eks_admin_serviceaccount.yml
    kubectl apply -f /tmp/eks_admin_clusterrolebinding.yml
  register: eks_admin_user_out
  changed_when: "'created' in eks_admin_user_out.stdout"

- name: Create Kubernetes nodes.
  include_tasks: createnodes.yml
